"""
Generate data quality report for StockFlowML.
"""

import sys
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data.data_loader import StockDataLoader
from src.utils.config import Config


def generate_data_quality_report(ticker: str = Config.DEFAULT_TICKER) -> str:
    """Generate markdown data quality report."""
    
    print(f"Generating data quality report for {ticker}...")
    
    # Load data
    loader = StockDataLoader(ticker=ticker)
    try:
        df = loader.download_data(use_fallback=True)
        df = loader.clean_data(validate=True)
    except Exception as e:
        return f"# Data Quality Report - FAILED\n\n**Error**: {e}\n"
    
    # Get validation stats
    stats = loader.validation_stats
    
    # Build report
    report = f"""# StockFlowML - Data Quality Report

> **Ticker**: {ticker}  
> **Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
> **Data Source**: {loader.data_source}

---

## Executive Summary

| Metric | Value |
|--------|-------|
| **Total Rows** | {stats['total_rows']} |
| **Rows Before Validation** | {stats['rows_before_validation']} |
| **Rows Removed** | {stats['rows_removed']} |
| **Data Quality** | {('[OK] EXCELLENT' if stats['rows_removed'] == 0 else '[!] ACCEPTABLE' if stats['rows_removed'] < stats['rows_before_validation'] * 0.05 else '[X] NEEDS REVIEW')} |

---

## Data Coverage

**Date Range**: {stats['date_range']['start']} to {stats['date_range']['end']}  
**Trading Days**: {stats['date_range']['trading_days']}

---

## Data Cleaning Summary

"""
    
    if stats['rows_removed'] > 0:
        report += f"!! **{stats['rows_removed']} rows removed** during validation:\n\n"
        for reason, count in stats['invalid_row_breakdown'].items():
            report += f"- {reason.replace('_', ' ').title()}: {count} rows\n"
        report += "\n"
    else:
        report += "** **No invalid rows detected** - data is clean!\n\n"
    
    report += f"""---

## Price Statistics (Close)

| Statistic | Value |
|-----------|-------|
| **Minimum** | {stats['price_stats']['close_min']:.2f} |
| **Maximum** | {stats['price_stats']['close_max']:.2f} |
| **Mean** | {stats['price_stats']['close_mean']:.2f} |
| **Std Dev** | {stats['price_stats']['close_std']:.2f} |

---

## Volume Statistics

| Statistic | Value |
|-----------|-------|
| **Minimum** | {stats['volume_stats']['min']:,} |
| **Maximum** | {stats['volume_stats']['max']:,} |
| **Mean** | {stats['volume_stats']['mean']:,.0f} |

---

## Data Contract Validation

[OK] **All validation checks passed**:

1. **Schema Validation**
   - Required columns present: Date, Open, High, Low, Close, Volume
   - Correct data types enforced
   
2. **Time Properties**
   - No duplicate timestamps
   - Data sorted chronologically (ascending)
   - Trading days only (weekdays)

3. **Financial Integrity**
   - All prices positive (> 0)
   - Volume non-negative (>= 0)
   - Valid OHLC relationships:
     - Low <= Open <= High
     - Low <= Close <= High
     - High >= Low

4. **Missing Data**
   - No missing OHLCV values
   - Data ready for feature engineering

---

## Known Limitations

1. **Data Source**: {loader.data_source}
   - Yahoo Finance may have gaps or errors
{"   - **Using snapshot data** - verify freshness before production" if loader.data_source == "CSV Fallback" else "   - Live data subject to API availability"}

2. **Market Coverage**: Daily frequency only
   - Intraday data not available
   - After-hours trading not captured

3. **Corporate Actions**: Not adjusted
   - Splits and dividends may cause price jumps
   - Consider using adjusted close for long-term analysis

4. **Data Lag**: Depends on source
   - Yahoo Finance: T+1 (next business day)
   - Snapshot: Check snapshot date

---

## Recommendations

### For Production Use

1. [OK] **Data validation passed** - safe to use for ML training
2. [!] Monitor data freshness (check latest date)
3. [!] Verify Yahoo Finance availability before scheduled retraining
4. [!] Keep snapshot data updated as fallback

### For Model Training

1. [OK] Use temporal train/test split (already implemented)
2. [OK] Feature engineering can proceed
3. [!] Consider additional filters:
   - Minimum volume threshold for liquidity
   - Outlier detection for price spikes
   - Adjust for stock splits if needed

---

## Audit Trail

**Validation Module**: `src/data/data_validation.py`  
**Data Loader**: `src/data/data_loader.py`  
**Configuration**: `src/utils/config.py`

All validation logic is explicit and auditable. No silent data fixes applied.

---

*Report generated by StockFlowML Data Quality Pipeline*  
*For questions, review `reports/data_quality_report.md`*
"""
    
    return report


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Generate data quality report')
    parser.add_argument('--ticker', default=Config.DEFAULT_TICKER, help='Stock ticker')
    args = parser.parse_args()
    
    report = generate_data_quality_report(args.ticker)
    
    # Save report
    output_path = Config.REPORTS_DIR / "data_quality_report.md"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(report)
    
    print(f"\nâœ“ Report saved to {output_path}")
    print("\nPreview:")
    print("="*70)
    print(report[:500] + "...")
