"""
Model evaluation module.
Calculates metrics and generates reports.
"""

import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)
from typing import Dict, Any
import logging
from datetime import datetime

from src.utils.config import Config

logger = logging.getLogger(__name__)


class ModelEvaluator:
    """Evaluate model performance and generate reports."""
    
    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray = None):
        """
        Initialize evaluator.
        
        Args:
            y_true: True labels
            y_pred: Predicted labels
            y_proba: Prediction probabilities (optional)
        """
        self.y_true = y_true
        self.y_pred = y_pred
        self.y_proba = y_proba
        self.metrics = {}
    
    def calculate_metrics(self) -> Dict[str, float]:
        """
        Calculate all evaluation metrics.
        
        Returns:
            Dictionary of metrics
        """
        logger.info("Calculating metrics...")
        
        self.metrics = {
            'accuracy': accuracy_score(self.y_true, self.y_pred),
            'precision': precision_score(self.y_true, self.y_pred, zero_division=0),
            'recall': recall_score(self.y_true, self.y_pred, zero_division=0),
            'f1_score': f1_score(self.y_true, self.y_pred, zero_division=0)
        }
        
        # Calculate confusion matrix
        cm = confusion_matrix(self.y_true, self.y_pred)
        self.metrics['confusion_matrix'] = cm.tolist()
        
        if len(cm) == 2:
            tn, fp, fn, tp = cm.ravel()
            self.metrics['true_negatives'] = int(tn)
            self.metrics['false_positives'] = int(fp)
            self.metrics['false_negatives'] = int(fn)
            self.metrics['true_positives'] = int(tp)
        
        return self.metrics
    
    def get_classification_report(self) -> str:
        """
        Get detailed classification report.
        
        Returns:
            Classification report as string
        """
        return classification_report(
            self.y_true, 
            self.y_pred,
            target_names=['Down (0)', 'Up (1)']
        )
    
    def generate_markdown_report(
        self, 
        model_type: str,
        train_metrics: Dict[str, float],
        test_metrics: Dict[str, float],
        dataset_info: Dict[str, Any]
    ) -> str:
        """
        Generate markdown report for metrics logging.
        
        Args:
            model_type: Type of model
            train_metrics: Training set metrics
            test_metrics: Test set metrics
            dataset_info: Information about the dataset
            
        Returns:
            Markdown formatted report
        """
        report = f"""# StockFlowML - Model Performance Report

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Dataset Information
- **Ticker:** {dataset_info.get('ticker', 'N/A')}
- **Total Samples:** {dataset_info.get('total_samples', 'N/A')}
- **Training Samples:** {dataset_info.get('train_samples', 'N/A')}
- **Test Samples:** {dataset_info.get('test_samples', 'N/A')}
- **Date Range:** {dataset_info.get('date_range', 'N/A')}

## Model: {model_type}

### Training Set Performance
- **Accuracy:** {train_metrics.get('accuracy', 0):.4f}
- **Precision:** {train_metrics.get('precision', 0):.4f}
- **Recall:** {train_metrics.get('recall', 0):.4f}
- **F1-Score:** {train_metrics.get('f1_score', 0):.4f}

### Test Set Performance
- **Accuracy:** {test_metrics.get('accuracy', 0):.4f} ⭐
- **Precision:** {test_metrics.get('precision', 0):.4f}
- **Recall:** {test_metrics.get('recall', 0):.4f}
- **F1-Score:** {test_metrics.get('f1_score', 0):.4f}

### Confusion Matrix (Test Set)
```
                Predicted Down  Predicted Up
Actual Down     {test_metrics.get('true_negatives', 0):>14}  {test_metrics.get('false_positives', 0):>12}
Actual Up       {test_metrics.get('false_negatives', 0):>14}  {test_metrics.get('true_positives', 0):>12}
```

## Interpretation

- **Accuracy** ({test_metrics.get('accuracy', 0):.2%}): Percentage of correct predictions
- **Precision** ({test_metrics.get('precision', 0):.2%}): When model predicts UP, how often is it correct?
- **Recall** ({test_metrics.get('recall', 0):.2%}): Of all actual UP days, how many did model catch?

## Next Steps

- Monitor model performance on new data
- Consider retraining when test accuracy drops below {train_metrics.get('accuracy', 0) * 0.9:.2%}
- Experiment with additional features (Level 2)

---
*This is an automated report generated by StockFlowML pipeline*
"""
        return report
    
    def save_markdown_report(
        self, 
        model_type: str,
        train_metrics: Dict[str, float],
        test_metrics: Dict[str, float],
        dataset_info: Dict[str, Any]
    ):
        """
        Generate and save markdown report.
        
        Args:
            model_type: Type of model
            train_metrics: Training metrics
            test_metrics: Test metrics
            dataset_info: Dataset information
        """
        report = self.generate_markdown_report(
            model_type, train_metrics, test_metrics, dataset_info
        )
        
        metrics_path = Config.get_metrics_path()
        metrics_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(metrics_path, 'w') as f:
            f.write(report)
        
        logger.info(f"Metrics report saved to {metrics_path}")


def main():
    """Main function for evaluation."""
    from src.models.train import StockTrendModel
    
    # Load processed data
    processed_path = Config.get_processed_data_path(Config.DEFAULT_TICKER)
    df = pd.read_csv(processed_path, parse_dates=['Date'])
    
    # Load trained model
    model = StockTrendModel(model_type='logistic')
    
    try:
        model.load_model()
    except FileNotFoundError:
        print("Model not found. Train model first using train.py")
        return
    
    # Prepare data
    X_train, X_test, y_train, y_test, train_df, test_df = model.prepare_data(df)
    
    # Get predictions
    train_preds = model.predict(X_train)
    test_preds = model.predict(X_test)
    
    # Evaluate
    train_evaluator = ModelEvaluator(y_train, train_preds)
    train_metrics = train_evaluator.calculate_metrics()
    
    test_evaluator = ModelEvaluator(y_test, test_preds)
    test_metrics = test_evaluator.calculate_metrics()
    
    # Print results
    print("\n" + "="*60)
    print("EVALUATION RESULTS")
    print("="*60)
    print("\nTraining Set:")
    print(train_evaluator.get_classification_report())
    print("\nTest Set:")
    print(test_evaluator.get_classification_report())
    
    # Dataset info
    dataset_info = {
        'ticker': Config.DEFAULT_TICKER,
        'total_samples': len(df),
        'train_samples': len(train_df),
        'test_samples': len(test_df),
        'date_range': f"{df['Date'].min().date()} to {df['Date'].max().date()}"
    }
    
    # Save markdown report
    test_evaluator.save_markdown_report(
        model_type='Logistic Regression',
        train_metrics=train_metrics,
        test_metrics=test_metrics,
        dataset_info=dataset_info
    )
    
    print(f"\n✓ Metrics saved to {Config.get_metrics_path()}")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main()
